{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hugotomita1201/yachay.ai_project/blob/main/deberta_grouped__class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X11ffI8wwpji"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEjDGEZH0cJ1",
        "outputId": "c681ff6c-cd0c-433c-83c4-cd98a7012295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type deberta-v2 to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing took 20.57912564277649 seconds\n",
            "torch.Size([38563, 512])\n",
            "torch.Size([9641, 512])\n",
            "Training started at Fri May  5 03:41:14 2023\n",
            "Epoch 1: Test loss=4.0953, Test accuracy=0.0695, Avg distance=1885.8982 km\n",
            "Epoch 2: Test loss=4.0804, Test accuracy=0.0674, Avg distance=1883.2386 km\n",
            "Epoch 3: Test loss=4.0711, Test accuracy=0.0701, Avg distance=1871.9803 km\n",
            "Epoch 4: Test loss=4.0696, Test accuracy=0.0667, Avg distance=1865.4635 km\n",
            "Epoch 5: Test loss=4.0653, Test accuracy=0.0683, Avg distance=1872.1178 km\n",
            "Training took 12874.169991493225 seconds\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n@misc{he2021debertav3,\\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \\n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\\n      year={2021},\\n      eprint={2111.09543},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "# %%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from transformers import BertConfig\n",
        "import pandas as pd\n",
        "import csv\n",
        "import time\n",
        "import pickle\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "#use gpu if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "df = pd.read_csv('df_grouped_class.csv')\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "model_name = 'microsoft/deberta-v3-small'\n",
        "\n",
        "config = BertConfig.from_pretrained(model_name)\n",
        "config.num_labels = 100  # number of regions for classification\n",
        "bert_model = AutoModel.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name, normalize=True)\n",
        "\n",
        "#bert_model = RobertaModel.from_pretrained(model_name, config=config)\n",
        "#tokenizer = BertweetTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# preprocess data with function\n",
        "\n",
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    earth_radius = 6371  # Earth radius in km\n",
        "    pi = torch.tensor(3.141592653589793, dtype=torch.float)\n",
        "\n",
        "    lat1, lon1, lat2, lon2 = [torch.tensor(x.to_numpy(), dtype=torch.float) * (pi / 180) for x in [lat1, lon1, lat2, lon2]]\n",
        "\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "\n",
        "    a = torch.sin(dlat / 2) ** 2 + torch.cos(lat1) * \\\n",
        "        torch.cos(lat2) * torch.sin(dlon / 2) ** 2\n",
        "    c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n",
        "\n",
        "    distance = earth_radius * c\n",
        "    return distance\n",
        "\n",
        "\n",
        "class HaversineLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HaversineLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        lat1, lon1 = y_pred[:, 0], y_pred[:, 1]\n",
        "        lat2, lon2 = y_true[:, 0], y_true[:, 1]\n",
        "\n",
        "        distance = haversine_distance(lat1, lon1, lat2, lon2)\n",
        "        loss = distance.mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "def preprocess_data(df, tokenizer):\n",
        "\n",
        "    # Filter and sample data\n",
        "    df = df[df['language'] == 0]\n",
        "\n",
        "    # Split into train and test sets\n",
        "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
        "        df['text'], df[['group']], test_size=0.2, random_state=42)\n",
        "    train_featuresx, test_featuresx, train_labelsx, test_labels_centroids = train_test_split(\n",
        "        df['text'], df[['group_centroid_lat', 'group_centroid_lng']], test_size=0.2, random_state=42)\n",
        "    \n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = tokenizer\n",
        "\n",
        "    # Tokenize the training and test text features\n",
        "    train_encodings = tokenizer(train_features.tolist(\n",
        "    ), padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "    test_encodings = tokenizer(test_features.tolist(\n",
        "    ), padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "\n",
        "    # Tokenize the training and test text features\n",
        "    train_input_ids = train_encodings['input_ids'].to(device)\n",
        "    test_input_ids = test_encodings['input_ids'].to(device)\n",
        "\n",
        "    # make attention masks to let know which tokens are real and which are padding\n",
        "    train_attention_mask = train_encodings['attention_mask'].to(device)\n",
        "    test_attention_mask = test_encodings['attention_mask'].to(device)\n",
        "\n",
        "    # Convert labels to PyTorch tensors with long datatype\n",
        "    train_labels = torch.tensor(\n",
        "        train_labels['group'].values, dtype=torch.long).to(device)\n",
        "    test_labels = torch.tensor(\n",
        "        test_labels['group'].values, dtype=torch.long).to(device)\n",
        "\n",
        "    return train_encodings, test_encodings, train_input_ids, test_input_ids, train_attention_mask, test_attention_mask, train_labels, test_labels, test_labels_centroids\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "# call on function\n",
        "train_encodings, test_encodings, train_input_ids, test_input_ids, train_attention_mask, test_attention_mask, train_labels, test_labels, test_labels_centroids = preprocess_data(\n",
        "     df, tokenizer)\n",
        "end_time = time.time()\n",
        "print(f\"Preprocessing took {end_time - start_time} seconds\")\n",
        "# print out the shape of the data\n",
        "\n",
        "print(train_input_ids.shape)\n",
        "print(test_input_ids.shape)\n",
        "\n",
        "# Define the BERT-based classification model\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_regions):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_regions)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_outputs = self.bert(\n",
        "            input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = bert_outputs[0][:, 0, :]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "\n",
        "num_regions = 100\n",
        "\n",
        "# Initialize the BERT-based classification model\n",
        "bert_classifier = BERTClassifier(bert_model, num_regions).to(device)\n",
        "\n",
        "# Define the optimizer and the loss function\n",
        "optimizer = AdamW(bert_classifier.parameters(), lr=1e-4, weight_decay=1e-9)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# %%\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "batch_size = 16\n",
        "\n",
        "# set a timer to see how long it takes to train the model\n",
        "\n",
        "start_time = time.time()\n",
        "# print starting time\n",
        "print(f\"Training started at {time.ctime(start_time)}\")\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(train_input_ids), batch_size):\n",
        "        input_ids_batch = train_input_ids[i:i+batch_size].to(device)\n",
        "        attention_mask_batch = train_attention_mask[i:i+batch_size].to(device)\n",
        "        region_ids_batch = train_labels[i:i+batch_size].to(device)\n",
        "\n",
        "        bert_classifier.zero_grad()\n",
        "\n",
        "        logits = bert_classifier(\n",
        "            input_ids_batch, attention_mask_batch).to(device)\n",
        "        loss = loss_fn(logits, region_ids_batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate the model on the test set after each epoch\n",
        "    test_batch_size = 16 \n",
        "\n",
        "    # Evaluate the model on the test set after each epoch\n",
        "    with torch.no_grad():\n",
        "      test_losses = []\n",
        "      test_accuracies = []\n",
        "      test_distances = []\n",
        "      test_predictions = []\n",
        "      test_actual_labels = []\n",
        "    \n",
        "\n",
        "      for i in range(0, len(test_input_ids), test_batch_size):\n",
        "        test_input_ids_batch = test_input_ids[i:i+test_batch_size].to(device)\n",
        "        test_attention_mask_batch = test_attention_mask[i:i+test_batch_size].to(device)\n",
        "        test_labels_batch = test_labels[i:i+test_batch_size].to(device)\n",
        "\n",
        "        test_logits_batch = bert_classifier(test_input_ids_batch, test_attention_mask_batch)\n",
        "        test_loss_batch = loss_fn(test_logits_batch, test_labels_batch)\n",
        "        \n",
        "        test_losses.append(test_loss_batch.item())\n",
        "\n",
        "        #append test predictions and labels to list for each epoch\n",
        "        test_predictions.append(torch.argmax(test_logits_batch, dim=1).cpu().numpy())\n",
        "        test_actual_labels.append(test_labels_batch.cpu().numpy())\n",
        "\n",
        "      #make arrays into dataframes to merge with df\n",
        "      test_predictions_df = pd.DataFrame(np.concatenate(test_predictions, axis=0).reshape(-1,1), columns=['group'])\n",
        "      test_actual_labels_df = pd.DataFrame(np.concatenate(test_actual_labels, axis=0).reshape(-1,1), columns=['group'])\n",
        "\n",
        "      #calculate centroids for predicted and labels groups\n",
        "      predicted_centroids = test_predictions_df.merge(df[['group', 'group_centroid_lat', 'group_centroid_lng']].drop_duplicates(subset=['group']), on='group', how='left')[['group_centroid_lat', 'group_centroid_lng']]\n",
        "      labels_centroids = test_actual_labels_df.merge(df[['group', 'group_centroid_lat', 'group_centroid_lng']].drop_duplicates(subset=['group']), on='group', how='left')[['group_centroid_lat', 'group_centroid_lng']]\n",
        "      \n",
        "      #calculate haversine distance based on predicted and labels centroids\n",
        "      haversine_distances = haversine_distance(predicted_centroids['group_centroid_lat'], predicted_centroids['group_centroid_lng'], labels_centroids['group_centroid_lat'], labels_centroids['group_centroid_lng'])\n",
        "    \n",
        "      avg_distance = sum(haversine_distances) / len(haversine_distances)\n",
        "\n",
        "      avg_test_loss = sum(test_losses) / len(test_losses)\n",
        "      test_accuracy = accuracy_score(test_actual_labels_df, test_predictions_df)\n",
        "      print(\n",
        "            f\"Epoch {epoch+1}: Test loss={avg_test_loss:.4f}, Test accuracy={test_accuracy:.4f}, Avg distance={avg_distance:.4f} km\")\n",
        "      \n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training took {end_time - start_time} seconds\")\n",
        "\n",
        "\n",
        "# %%\n",
        "torch.save(bert_classifier.state_dict(),\n",
        "           'bert_classifier_for_grouped_data.pth')\n",
        "\n",
        "'''\n",
        "@misc{he2021debertav3,\n",
        "      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n",
        "      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n",
        "      year={2021},\n",
        "      eprint={2111.09543},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.CL}\n",
        "}\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPFTGf12TSCIYAoYjQdmf5k",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}